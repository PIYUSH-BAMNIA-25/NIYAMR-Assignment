{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Objective*** :\n",
        "## *Build a mini AI agent that can read, summarise, and analyse the Universal Credit Act 2025 and return a structured JSON report.*\n",
        "\n",
        "---\n",
        "# **Step's Followed** :\n",
        "1. ***Installation Of Specific Libraries/Setting Up the Environment***\n",
        "2. ***Loding The Required LLM and Vector Models***\n",
        "3. ***Making a Single Pipeline for result***\n",
        "\n",
        "---\n",
        "## *Input File* : *Universal Credit Act 2025*\n",
        "## *Output File* : *a structured JSON report*\n",
        "\n",
        "---\n",
        "# ***Details about the libraries Used*** :\n",
        "###1. **Docling** : *Docling is the current one of the best data extractor which good at extracting compllex Data like Table , Complex act Structure etc. Which is best for this task*\n",
        "###2. **Langchain** : *For the PAG pipleline*\n",
        "###3. **ChromaDB** : *Best to store the vector Database*\n",
        "###4. **Transformers**: *Used so the LLM model Run Locally*\n",
        "###5. **BitsAndBytes** : *I used because I have a low end setup. [ Note: Its is optional only use when you have a setup with low memory and GPU ]*\n",
        "\n",
        "# ***Details about the LLM models Used*** :\n",
        "###1. **'NOMIC-EMBED-TEST-v1.5' [ Embedded Model ]** : *I used This model because it has a 8192 context window ehich is good for handling long section or long data perfectly then other model.*\n",
        "###2. **'QWEN 2.5-7B ( INSTRUCT VERSION )' [ Generator Model ]** : *I used this model because it is currently one of the best 7B model and has a memory to store long context which is best for this task*\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_kiDqpaAQZ_W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK7THVp7QORS"
      },
      "outputs": [],
      "source": [
        "print(\"‚è≥ Installing Required Libararies need For this Task. Please Wait It Take some Time.....\\n\")\n",
        "# Libraries for Data Extraction From the Raw Pdf\n",
        "!pip install -q -U docling pydantic\n",
        "\n",
        "# Libraries For the Vector Data base and LLM models\n",
        "!pip install -q -U langchain langchain-chromadb langchain-community langchain-huggingface chromadb\n",
        "!pip install -q -U bitsandbytes accelerate peft transformers\n",
        "\n",
        "print(\"‚úÖ All libararies Install Sucessfully .\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "def load_ai_models():\n",
        "\n",
        "    '''\n",
        "    This Function Is basically made So we can load the LLM Models for both The Verctor DB ( Brain )\n",
        "    and the LLM ( Generator ).\n",
        "\n",
        "    Vector DB model ( nomic-embed-text-v1.5 ) : In simple Words Vector DB model is model which is used to store the Raw Data\n",
        "                                                JSON file in a numarical Vector which is then used for LLM's.\n",
        "    LLM Model ( Qwen 2.5-7B ( Instruct Version ) ) : In simple Words LLM model is Used to generate the reports,Its take data from the Brain ( Vector DB)\n",
        "                                                     as context and generate the report.\n",
        "    '''\n",
        "\n",
        "    print(\"üß† Loading the LLM Model for the Vector DataBase....\")\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
        "        model_kwargs={\"trust_remote_code\": True, \"device\": \"cuda\"}\n",
        "    )\n",
        "\n",
        "    print(\"ü§ñ Loading the Generator : Qwen2.5-7B-Instruct ....)\n",
        "    '''\n",
        "    Note : Below bnb_config is optional only use when you have a setup with low memory and GPU.\n",
        "    '''\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config, # use only when you make a bnb_config\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Create the text generation pipeline\n",
        "    text_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=2048,\n",
        "        temperature=0.1,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "    print(\"‚úÖ Both Model loaded Sucessfull. Ready to use !\")\n",
        "    return embedding_model, llm\n",
        "\n",
        "# Execute the loader\n",
        "em_model, llm = load_ai_models()"
      ],
      "metadata": {
        "id": "fgS5bOznXul6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from google.colab import files\n",
        "from docling.document_converter import DocumentConverter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "\n",
        "# This Function is used to extract the Result From the LLM generated Output\n",
        "def clean_json_response(text, expected_type=\"dict\"):\n",
        "    \"\"\"\n",
        "    Sanitizes the LLM output to extract valid JSON, ignoring conversational text.\n",
        "    Handles both Dictionaries {} and Lists [].\n",
        "    \"\"\"\n",
        "    # Strategy 1: Look for Markdown Code Blocks (e.g., ```json ... ```)\n",
        "    code_block = re.search(r'```(?:json)?\\s*(\\{|\\[)(.*?)(\\}|\\])\\s*```', text, re.DOTALL)\n",
        "    if code_block:\n",
        "        clean_str = code_block.group(1) + code_block.group(2) + code_block.group(3)\n",
        "        try:\n",
        "            return json.loads(clean_str)\n",
        "        except:\n",
        "            pass # If this Parsing Failed ( like there is not markdown {} then it auto swift to the other one's.)\n",
        "\n",
        "    # Strategy 2: Raw Regex Search\n",
        "    if expected_type == \"list\":\n",
        "        # Find the largest outer bracket [] containing a curly brace {}\n",
        "        match = re.search(r'(\\[\\s*\\{.*\\}\\s*\\])', text, re.DOTALL)\n",
        "    else:\n",
        "        # Find the largest outer curly brace {}\n",
        "        match = re.search(r'(\\{.*\\})', text, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(0))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return [] if expected_type == \"list\" else {}\n",
        "\n",
        "\n",
        "# This is the Main function Its handle all task.\n",
        "'''\n",
        "Working of the Function :\n",
        "1. Upload PDF -> 2. Docling Extraction -> 3. Chroma Indexing -> 4. Qwen Analysis -> 5. Output\n",
        "'''\n",
        "\n",
        "def run_legal_agent():\n",
        "    # --- STEP 1: UPLOAD ---\n",
        "    print(\"\\nüìÇ Please Upload the PDF....\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No file uploaded. Exiting.\")\n",
        "        return\n",
        "    pdf_filename = next(iter(uploaded))\n",
        "\n",
        "    # --- STEP 2: EXTRACTION (VISION) ---\n",
        "    print(f\"üöÄ PHASE 1: Extracting structure from {pdf_filename}...\")\n",
        "    # For this Stage we use the Docling , Because its is better in Table extraction then the normal OCR.\n",
        "    converter = DocumentConverter()\n",
        "    result = converter.convert(pdf_filename)\n",
        "    full_markdown = result.document.export_to_markdown()\n",
        "    print(f\"   -> Extraction successful. Document length: {len(full_markdown)} chars.\")\n",
        "\n",
        "    # --- STEP 3: INDEXING (RAG) ---\n",
        "    print(\"üíæ PHASE 2: building RAG Memory (Vector Database)...\")\n",
        "    # Split text into chunks so the AI can \"read\" specific pages\n",
        "    text_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    documents = text_splitter.create_documents([full_markdown], metadatas=[{\"source\": pdf_filename}])\n",
        "\n",
        "    # Index documents into ChromaDB using Nomic Embeddings\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=em_model,\n",
        "        persist_directory=\"./chroma_final\"\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
        "\n",
        "    # --- STEP 4: CONTEXT RETRIEVAL ---\n",
        "    # We ask the database for specific sections to feed the AI\n",
        "    print(\"üîç PHASE 3: Retrieving key legal sections...\")\n",
        "    rag_docs = retriever.invoke(\"definitions eligibility penalties payments obligations enforcement uplift percentage table\")\n",
        "    rag_text = \"\\n\\n\".join([d.page_content for d in rag_docs])\n",
        "\n",
        "    # --- STEP 5: TASK 3 (SECTION EXTRACTION) ---\n",
        "    print(\"ü§ñ PHASE 4: Generating Task 3 Report (Extraction)...\")\n",
        "\n",
        "    prompt_task3 = f\"\"\"\n",
        "    [CONTEXT]\n",
        "    {rag_text}\n",
        "    [END CONTEXT]\n",
        "\n",
        "    You are a legal AI analyst. Analyze the text above from the 'Universal Credit Act 2025'.\n",
        "\n",
        "    IMPORTANT CONTEXT:\n",
        "    This is an AMENDMENT ACT. It primarily updates rates and definitions.\n",
        "    If a specific section (like Penalties or Record Keeping) is not found, do NOT invent one.\n",
        "    Instead, write: \"Not specified in this Amendment (refers to Principal Regulations).\"\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. For 'payments', specifically look for the 'Relevant uplift percentage' table (e.g. 2.3%, 3.1%).\n",
        "    2. For 'definitions', extract 'pre-2026 claimant' and 'severe conditions'.\n",
        "    3. Use '¬£' for currency.\n",
        "\n",
        "    OUTPUT: Return ONLY valid JSON.\n",
        "    {{\n",
        "      \"definitions\": \"...\",\n",
        "      \"obligations\": \"...\",\n",
        "      \"responsibilities\": \"...\",\n",
        "      \"eligibility\": \"...\",\n",
        "      \"payments\": \"...\",\n",
        "      \"penalties\": \"...\",\n",
        "      \"record_keeping\": \"...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response_3 = llm.invoke(prompt_task3)\n",
        "    task3_json = clean_json_response(response_3, expected_type=\"dict\")\n",
        "\n",
        "    # --- STEP 6: TASK 4 (RULE CHECKS) ---\n",
        "    print(\"ü§ñ PHASE 5: Performing Task 4 (Rule Verification)...\")\n",
        "\n",
        "    prompt_task4 = f\"\"\"\n",
        "    [CONTEXT]\n",
        "    {rag_text}\n",
        "    [END CONTEXT]\n",
        "\n",
        "    Perform a strict compliance check on the 'Universal Credit Act 2025'.\n",
        "    Return a JSON LIST of exactly 6 objects representing the rules below.\n",
        "\n",
        "    RULES TO CHECK:\n",
        "    1. Act must define key terms (Pass if 'pre-2026 claimant' is defined)\n",
        "    2. Act must specify eligibility criteria (Pass if criteria for severe conditions is mentioned)\n",
        "    3. Act must specify responsibilities of authority (Pass if Secretary of State powers are mentioned)\n",
        "    4. Act must include enforcement or penalties (Fail if no specific penalties in this text)\n",
        "    5. Act must include payment calculation (Pass if uplift percentages are found)\n",
        "    6. Act must include record-keeping (Fail or Pass based on 'Information Requirements')\n",
        "\n",
        "    OUTPUT: Return ONLY valid JSON List.\n",
        "    [\n",
        "      {{ \"rule\": \"Act must define key terms\", \"status\": \"pass\", \"evidence\": \"...\", \"confidence\": 100 }},\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    response_4 = llm.invoke(prompt_task4)\n",
        "    task4_json = clean_json_response(response_4, expected_type=\"list\")\n",
        "\n",
        "    # --- STEP 7: MERGE & DOWNLOAD ---\n",
        "    final_output = {\n",
        "        \"task_3_extraction\": task3_json,\n",
        "        \"task_4_rule_checks\": task4_json\n",
        "    }\n",
        "\n",
        "    filename = \"Final_Submission_Output.json\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(final_output, f, indent=4)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üéâ SUCCESS: Report Generated & Saved.\")\n",
        "    print(\"=\"*50)\n",
        "    print(json.dumps(final_output, indent=2))\n",
        "\n",
        "    files.download(filename)\n",
        "\n",
        "# Start the Agent\n",
        "run_legal_agent()"
      ],
      "metadata": {
        "id": "ChgIkzt0mPSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}